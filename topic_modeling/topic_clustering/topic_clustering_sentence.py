# -*- coding: utf-8 -*-
"""topic_cluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12qqX9LyJb_hk6_yAeL1fnBgqbizxAelU
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/blockchain/patents_keywords.csv", encoding='utf-8') # news_keywords.csv / papers_keywords.csv / patents_keywords.csv
#data = df['patent_keywords'] #'patent_keywords'
print(df)

# dmr = df[['topic number','patent_keywords']].iloc[0:12]
# print(dmr)
# bert = df[['topic number','patent_keywords']].iloc[12:]
# print(bert)


# news / papers / patents 선택해서 주석제외하고 돌리면 됨 
# news
# dmr = df['dmr_news_keywords'][0:12]
# print(dmr)
# lda = df['lda_news_keywords'][0:12]
# print(lda)

# papers
# dmr = df['dmr_papers_keywords'][0:8]
# print(dmr)
# lda = df['lda_papers_keywords'][0:8]
# print(lda)

# patents
dmr = df['dmr_patents_keywords'][0:14]
print(dmr)
lda = df['lda_patents_keywords'][0:14]
print(lda)

sentence1 = dmr.to_list()
print(sentence1)
sentence2 = lda.to_list()

corpus = sentence1 + sentence2
print(corpus)

!pip install sentence_transformers

"""### Cosine similarity"""

# from sentence_transformers import SentenceTransformer, util
# model = SentenceTransformer('paraphrase-MiniLM-L6-v2') #'sentence-t5-base' #'all-MiniLM-L6-v2' #'all-mpnet-base-v2' # 'paraphrase-MiniLM-L6-v2' #sentence-t5-base


# # Compute embedding for both lists
# embeddings1 = model.encode(sentence1, convert_to_tensor=True)
# embeddings2 = model.encode(sentence2, convert_to_tensor=True)

# # Compute cosine-similarities
# cosine_scores = util.cos_sim(embeddings1, embeddings2)
# print(cosine_scores)

# # for i in range(len(sentence1)):
# #   print(f'{sentence1[i]} - {sentence2[i]} - Score: {cosine_scores[i][i]:.4f}')
# # #Output the pairs with their score
# for i in range(len(sentence1)):
#     for j in range(len(sentence2)):
#         result = "{} \t\t {} \t\t {:.4f}".format(sentence1[i], sentence2[j], cosine_scores[i][j])
#         print(result)


# # cosine similarity matrix
# # import the cosine_similarity library
# from sklearn.metrics.pairwise import cosine_similarity

# df_cosine=pd.DataFrame(cosine_similarity(embeddings1, embeddings2, dense_output=True))
# df_cosine
# print(df_cosine)
# df_cosine.to_csv('/content/drive/MyDrive/Colab Notebooks/blockchain/news_cosine_similarity.csv')


# # Highst similarity
# # Find the pairs with the highest cosine similarity scores
# pairs = []
# for i in range(len(cosine_scores)-1):
#     for j in range(i+1, len(cosine_scores)):
#         pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})


# # Sort scores in decreasing order
# pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)
# print(pairs)

# for pair in pairs[0:30]:
#     i, j = pair['index']
#     print(f"{sentence1[i]} - {sentence2[j]} - Score: {pair['score']:.4f}")

"""### AgglomerativeClustering (cosine_similarity 사용)"""

# https://stackoverflow.com/questions/55619176/how-to-cluster-similar-sentences-using-bert #참고 링크

"""
This is a simple application for sentence embeddings: clustering

Sentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.
"""
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
import numpy as np

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2') #'sentence-t5-base' #'all-MiniLM-L6-v2' #'all-mpnet-base-v2' # 'paraphrase-MiniLM-L6-v2'

# Corpus with example sentences
corpus_embeddings = embedder.encode(corpus)

# Normalize the embeddings to unit length
corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)

# Perform clustering
# n_clusters 랑 distance_threshold 둘 중 하나만 쓸 수 있음 
# n_clusters = 12
clustering_model = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='average', distance_threshold=0.6) # distance_threshold=0.4, distance_threshold=1.5 등
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = {}
for sentence_id, cluster_id in enumerate(cluster_assignment):
    if cluster_id not in clustered_sentences:
        clustered_sentences[cluster_id] = []

    clustered_sentences[cluster_id].append(corpus[sentence_id])
#print(clustered_sentences)

for i, cluster in clustered_sentences.items():
    print("Cluster ", i+1)
    print(cluster)
    print("")

# topic_num, keywords -> dataframe과 pickle형태로 저장하기

topic_cluster={}
topic_num = []
clustering = []
for i, cluster in clustered_sentences.items(): #items()쓰면 key와 value 쌍을 얻을 수 있음 
    #print("Cluster ", i+1)
    #print(cluster)
    str = " ".join(set(cluster)) # keywords간의 중복제거 하기 싫으면 set 없애면 됨 
    #print(str)
    clustering.append(str)
    num = i+1 #f"topic {i+1}" 
    topic_num.append(num)
    topic_cluster[num] = str
topic_keywords = topic_cluster
#print(topic_keywords) #key,value 형태 # ex) {'topic 1': 'cryptocurrencies crypto  services cryptocurrencies price', 'topic 2': 'government trade countries china economy growth sector industry country innovation nfts art game auction artists tokens games sale metaverse nft', 'topic 3': 'university health research students school education work program city science insider trends industry reports media book coverage report intelligence insiders', 'topic 4': 'security sanctions department government money hackers law court states case capital startups startup venture investment fund funding ventures fintech firm', 'topic 5': 'facebook law regulators tax government policy rules libra mr president university students school event research program director education science city', 'topic 6': 'nfts art work game media money something york thats things health supply group products logistics food industry sales customers vehicles', 'topic 7': 'services startups capital startup tech venture investment software platform firm law security court money department hackers enforcement case transactions fraud', 'topic 8': 'application transaction network images tables charts states office abstract device stock stocks shares growth cent investment markets price quarter index', 'topic 9': 'banks payments payment services banking currency money transactions credit customers mr things work money something thats lot media dont york', 'topic 10': 'energy food power industry mining supply oil gas electricity carbon crypto assets securities exchange asset trading regulators cryptocurrencies industry exchanges'}
#print(topic_keywords.keys())
#print(topic_keywords[1])
topic_num = topic_num
keywords = clustering
#print(keywords)


# dataframe 형태로 저장 
df1 = {'topic_num' : topic_num, 'keywords' : keywords}
df2 = pd.DataFrame(df1)
print(df2)
df2.to_csv('/content/drive/MyDrive/Colab Notebooks/blockchain/patents_topic_clustering.csv', index=False)


# picKle 형태로 저장 
import pickle
with open('/content/drive/MyDrive/Colab Notebooks/blockchain/patents_topic_clustering.pkl', "wb") as file:
    pickle.dump(topic_keywords, file)

df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/blockchain/patents_topic_clustering.pkl') #어떻게 생겼나 확인
print(df)

"""### dendrogram 그리기"""

# dendrogram 그리기 
# import libraries
from scipy.cluster.hierarchy import dendrogram

# call fit method with array of sample coordinates passed as a parameter
trained_model = clustering_model.fit(corpus_embeddings)

# A method for generating dendrogram
def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)


# plot dendrogram to visualize clusters
plot_dendrogram(trained_model)

# Dendrogram for Hierarchical Clustering
import scipy.cluster.hierarchy as shc
from matplotlib import pyplot
pyplot.figure(figsize=(10, 7))  
pyplot.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(corpus_embeddings, method='average'))

"""### Clustering - Performance """

# Agglomerative clustering
from numpy import unique
from numpy import where
from sklearn.cluster import AgglomerativeClustering
from matplotlib import pyplot
from sklearn.metrics import silhouette_score
from sklearn.metrics import calinski_harabasz_score
from sklearn.metrics import davies_bouldin_score

# define the model
#model = AgglomerativeClustering(n_clusters=4)
# fit model and predict clusters
yhat = clustering_model.fit(corpus_embeddings)
yhat_2 = clustering_model.fit_predict(corpus_embeddings)
# retrieve unique clusters
clusters = unique(yhat)
# Calculate cluster validation metrics
score_AGclustering_s = silhouette_score(corpus_embeddings, yhat.labels_, metric='cosine')
score_AGclustering_c = calinski_harabasz_score(corpus_embeddings, yhat.labels_)
score_AGclustering_d = davies_bouldin_score(corpus_embeddings, yhat_2)
print('Silhouette Score: %.4f' % score_AGclustering_s)
print('Calinski Harabasz Score: %.4f' % score_AGclustering_c)
print('Davies Bouldin Score: %.4f' % score_AGclustering_d)

# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html
# import matplotlib.pyplot as plt
# import numpy as np

# from sklearn.cluster import AgglomerativeClustering
# from sklearn.metrics import pairwise_distances

# np.random.seed(0)

# # Generate waveform data
# n_features = 2000
# t = np.pi * np.linspace(0, 1, n_features)


# def sqr(x):
#     return np.sign(np.cos(x))


# X = list()
# y = list()
# for i, (phi, a) in enumerate([(0.5, 0.15), (0.5, 0.6), (0.3, 0.2)]):
#     for _ in range(30):
#         phase_noise = 0.01 * np.random.normal()
#         amplitude_noise = 0.04 * np.random.normal()
#         additional_noise = 1 - 2 * np.random.rand(n_features)
#         # Make the noise sparse
#         additional_noise[np.abs(additional_noise) < 0.997] = 0

#         X.append(
#             12
#             * (
#                 (a + amplitude_noise) * (sqr(6 * (t + phi + phase_noise)))
#                 + additional_noise
#             )
#         )
#         y.append(i)

# X = np.array(X)
# y = np.array(y)

# n_clusters = 10

# labels = ('topic 1', 'topic 2', 'topic 3', 'topic 4', 'topic 5', 'topic 6', 'topic 7', 'topic 8', 'topic 9', 'topic 10')

# # Plot the ground-truth labelling
# plt.figure()
# plt.axes([0, 0, 1, 1])
# for l, c, n in zip(range(n_clusters), "rgb", labels):
#     lines = plt.plot(X[y == l].T, c=c, alpha=0.5)
#     lines[0].set_label(n)

# plt.legend(loc="best")

# plt.axis("tight")
# plt.axis("off")
# plt.suptitle("Ground truth", size=20)


# # Plot the distances
# for index, metric in enumerate(["cosine", "euclidean", "cityblock"]):
#     avg_dist = np.zeros((n_clusters, n_clusters))
#     plt.figure(figsize=(5, 4.5))
#     for i in range(n_clusters):
#         for j in range(n_clusters):
#             avg_dist[i, j] = pairwise_distances(
#                 X[y == i], X[y == j], metric=metric
#             ).mean()
#     avg_dist /= avg_dist.max()
#     for i in range(n_clusters):
#         for j in range(n_clusters):
#             plt.text(
#                 i,
#                 j,
#                 "%5.3f" % avg_dist[i, j],
#                 verticalalignment="center",
#                 horizontalalignment="center",
#             )

#     plt.imshow(avg_dist, interpolation="nearest", cmap=plt.cm.gnuplot2, vmin=0)
#     plt.xticks(range(n_clusters), labels, rotation=45)
#     plt.yticks(range(n_clusters), labels)
#     plt.colorbar()
#     plt.suptitle("Interclass %s distances" % metric, size=18)
#     plt.tight_layout()


# # Plot clustering results
# for index, metric in enumerate(["cosine", "euclidean", "cityblock"]):
#     model = AgglomerativeClustering(
#         n_clusters=n_clusters, linkage="average", affinity=metric
#     )
#     model.fit(X)
#     plt.figure()
#     plt.axes([0, 0, 1, 1])
#     for l, c in zip(np.arange(model.n_clusters), "rgbk"):
#         plt.plot(X[model.labels_ == l].T, c=c, alpha=0.5)
#     plt.axis("tight")
#     plt.axis("off")
#     plt.suptitle("AgglomerativeClustering(affinity=%s)" % metric, size=20)


# plt.show()



