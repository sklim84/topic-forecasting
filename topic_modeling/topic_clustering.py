# -*- coding: utf-8 -*-
"""cosine_similarity_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C_3V058Fhv2gSDMRhD_l-qQu7xHKCj4-
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/blockchain/news_keywords.csv", encoding='utf-8')
#data = df['patent_keywords'] #'patent_keywords'
print(df)

df.shape

df.columns

# dmr = df[['topic number','patent_keywords']].iloc[0:12]
# print(dmr)
# bert = df[['topic number','patent_keywords']].iloc[12:]
# print(bert)

dmr = df['dmr_news_keywords'][0:12]
print(dmr)
lda = df['lda_news_keywords'][0:12]
print(lda)

sentence1 = dmr.to_list()
sentence2 = lda.to_list()


!pip install sentence_transformers

from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('sentence-t5-base') #all-MiniLM-L6-v2

#Compute embedding for both lists
embeddings1 = model.encode(sentence1, convert_to_tensor=True)
embeddings2 = model.encode(sentence2, convert_to_tensor=True)

#Compute cosine-similarities
cosine_scores = util.cos_sim(embeddings1, embeddings2)
#print(cosine_scores)

# for i in range(len(sentence1)):
#   print(f'{sentence1[i]} - {sentence2[i]} - Score: {cosine_scores[i][i]:.4f}')
# #Output the pairs with their score
for i in range(len(sentence1)):
    for j in range(len(sentence2)):
        result = "{} \t\t {} \t\t {:.4f}".format(sentence1[i], sentence2[j], cosine_scores[i][j])
        print(result)

"""###COSINE SIMILARITY MATRIX"""

# # import the cosine_similarity library
# from sklearn.metrics.pairwise import cosine_similarity

# df_cosine=pd.DataFrame(cosine_similarity(embeddings1, embeddings2, dense_output=True))
# df_cosine
# df_cosine.to_csv('/content/drive/MyDrive/Colab Notebooks/blockchain/cosine_similarity.csv')

"""###Highest similarity"""

# #Highest similarity 
# #Find the pairs with the highest cosine similarity scores
# pairs = []
# for i in range(len(cosine_scores)-1):
#     for j in range(i+1, len(cosine_scores)):
#         pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})

# #Sort scores in decreasing order
# pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)

# for pair in pairs[0:30]:
#     i, j = pair['index']
#     print(f"{sentence1[i]} - {sentence2[j]} - Score: {pair['score']:.4f}")

"""###Clustering"""

#클러스터링
from sklearn.cluster import KMeans

# embeddings = model.encode(docs, convert_to_tensor=True)
embeddings1 = model.encode(sentence1, convert_to_tensor=True)
embeddings2 = model.encode(sentence2, convert_to_tensor=True)

num_clusters = 10

k_means = KMeans(n_clusters=num_clusters)
k_means.fit(embeddings1, embeddings2)

cluster_assignment = k_means.labels_

cluster_assignment


# 클러스터 개수 만큼 문장을 담을 리스트 초기화
clustered_sentences = [[] for _ in range(num_clusters)]

# 클러스터링 결과를 돌며 각 클러스터에 맞게 문장 삽입
for sentence_id, cluster_id in enumerate(cluster_assignment):
    clustered_sentences[cluster_id].append(sentence1[sentence_id])
    clustered_sentences[cluster_id].append(sentence2[sentence_id])
    # clustered_sentences[cluster_id].append(docs[sentence_id])

# 원래 코드 
# for i, cluster in enumerate(clustered_sentences):
#     result = "\n".join(cluster)
#     print(f"< topic {i+1} >\n {result}\n")


topic_cluster={}
for i, cluster in enumerate(clustered_sentences):
    result = " ".join(cluster)
    #result = result.replace(" ",",")
    topic = f"topic {i+1}"
    topic_cluster[topic] = result 

topic_10 =  topic_cluster
#print(topic_10) #key,value 형태 # ex) {'topic 1': 'cryptocurrencies crypto  services cryptocurrencies price', 'topic 2': 'government trade countries china economy growth sector industry country innovation nfts art game auction artists tokens games sale metaverse nft', 'topic 3': 'university health research students school education work program city science insider trends industry reports media book coverage report intelligence insiders', 'topic 4': 'security sanctions department government money hackers law court states case capital startups startup venture investment fund funding ventures fintech firm', 'topic 5': 'facebook law regulators tax government policy rules libra mr president university students school event research program director education science city', 'topic 6': 'nfts art work game media money something york thats things health supply group products logistics food industry sales customers vehicles', 'topic 7': 'services startups capital startup tech venture investment software platform firm law security court money department hackers enforcement case transactions fraud', 'topic 8': 'application transaction network images tables charts states office abstract device stock stocks shares growth cent investment markets price quarter index', 'topic 9': 'banks payments payment services banking currency money transactions credit customers mr things work money something thats lot media dont york', 'topic 10': 'energy food power industry mining supply oil gas electricity carbon crypto assets securities exchange asset trading regulators cryptocurrencies industry exchanges'}
#print(topic_10.keys())
#print(topic_10['topic 1'])


# picKle 형태로 저장 
import pickle
with open('/content/drive/MyDrive/Colab Notebooks/blockchain/news_topic_clustering.pkl', "wb") as file:
    pickle.dump(topic_10, file)

df = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/blockchain/news_topic_clustering.pkl') #어떻게 생겼나 확인
#print(df)


# dataframe 형태로 저장 
topic_num = ['topic 1', 'topic 2', 'topic 3', 'topic 4', 'topic 5', 'topic 6', 'topic 7', 'topic 8', 'topic 9', 'topic 10']

keywords = []
for i in range(10): # 10 = topic개수 
  value = topic_10[f"topic {i+1}"]
  keywords.append(value)
#print(keywords)

df1 = {'topic_num' : topic_num, 'keywords' : keywords}
df2 = pd.DataFrame(df1)
df2.to_csv('/content/drive/MyDrive/Colab Notebooks/blockchain/news_topic_clustering.csv', index=False)

#print(df2)







